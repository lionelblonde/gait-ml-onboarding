\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}   % For including images
\usepackage{amsmath}    % For mathematical equations
\usepackage{amssymb}    % For mathematical symbols
\usepackage{hyperref}   % For hyperlinks in the document
\usepackage{geometry}   % For setting page margins
\geometry{margin=1in}
\usepackage{cite}       % For managing citations

% Document Information
\title{\textsc{Gait} Project $n+1$}
\author{Lionel Blond\'e}
% \date{\today}

\begin{document}

% Title Page
\maketitle

% Table of Contents
\tableofcontents

% Introduction
\section{Introduction}
\label{s:intro}

In this document, we prioritize conciseness.
The objective is to lay out the various options in have in addressing what we set out to tackle
in this $n+1$ iteration of the \textsc{Gait} project.

To have a detailed account of \textit{what} the project sets out to tackle, please refer to the
proposal, which goes into the motivations and plans in depth.
In here, we lay out the variety of options we have to address the raised research questions.
This exhibition entails general approaches and methods, but also elaborates on the technical
solutions.

The objective is to have a clear picture not only of the research landscape in the area
but also to surface the conceptual and technical challenges that are inherent to the area.
While the \textit{area} in question might have been phrased as ``gait modeling'' in previous
iterations, in this $n+1$ iteration, we formulate is as
\emph{learning locomotion controllers}.

The task of learning how a control policy that knows how to walk \textit{when embodied physically
in the real world} is challenging for a multiplicity of reasons
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}.

We do not aim for embodied deployment.
What we want however is to learn controllers that are able to solve complex locomotion tasks
\textit{in silico}, \textit{i.e.}~in simulation.
While we wish to make use of such learned behaviors to derive insights and guide decisions in real
world tasks (such as surgical assistance), we do not aim to embody these in live robots.

An important point to make however is that most if not all the works in
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}
make use of a simulated world in order to devise the controllers that they deploy in the real
world. In that sense, the research progress carried out during this project could therefore be
transferable and readily extended to research endeavors targeted at embodying gait controllers
in hardware. The difficulty in the deploying such behavior is in closing the gap between the
simulated and real world: the ``sim-to-real'' gap. Despite not having to deal with this gap due
to the lack of deployment need \text{per se}, we face the gap when we have to deal with patient
data. Indeed, data recorded by physicians (\textit{e.g.} motion capture markers) do not live in
the same space as the locomotion simulators at our disposal. Using such data to learn controllers
in simulation required an alignment system that either
\textit{(i)} embeds the markers in the simulation space, or
\textit{(ii)} morphs the simulation space to the world the markers live in.
The latter corresponds to the real world.
Going for option \textit{(ii)} was the goal of iteration $n$ of the \textsc{Gait} project.
This alignment of the simulator with the patient's morphology was lead by the \textsc{BioRob}
team at EPFL. The actual technical solution changed over the course of the project as team
members were coming and going. In the end, the simulation suite that allowed for the flexibily
needed to close the gap between patient data and \textit{in silico} world was
OpenSim \cite{Seth2011-ru}. It stands as a viable option.

Going back a few steps, consider the task of learning a locomotion controller in simulation.
In broad lines, the task requires the researcher practitioner to
\textit{(a)} choose or design a \emph{locomotion simulator}
\textit{(b)} choose or design a \emph{learning signal}
\textit{(c)} choose or design a \emph{learning approach}
\textit{(d)} choose or design a \emph{learning algorithm}
These axes or choice or design are themselves multi-faceted, and this document will attempt to
give the account of the options we have for each.
Note, ``choose'' refers to the selection and use of an already existing solution,
while ``design'' refers to the deliberate creation of a new solution.
As it stands, such a system is therefore highly modular.
Progress can be made on any combination of the four axes above.
How these axes are formulated is flawed in the sense that these axes are not independent from
each other. For example, the approach chosen determines to an extent the classes of algorithms,
or trait thereof, that one might opt for.
Still, it provides a salient language for us to navigate how to tackle the many-sided task. 

Due to the interactions between these axes, the remainder will not catalog the four axes one by
one, giving suggestions for each independently. Instead, we will consider the various learning
approaches that can be considered to solve the task, and expand from there as we go with options.
Organizing the exhibition as such hopefully gives a better account of those options compared to
a catalog with bi-directional links all over the place.

\section{Landscape}
\label{s:landscape}

By \textit{simulator}, we refer to a piece of software that our learning agent interact with in
the course of its learning process. A simulator is what facilitates a \emph{simulation}, which
Sheldon M. Ross defines in \cite{Ross2022-ra} as the imitation of the operation of a real-world
process or system over time. In our case, the process we are interested in is the human gait.
Formally, the simulator is modelled as a Markov Decision Process or MDP \cite{Puterman1994-pf}.
The latter consists of a tuple containing a state space (the sitations the agent finds itself in),
an action space (the decisions the agent might make), a dynamics model that governs the physics of
the world in which the agent interacts, and a reward process. MDPs are used in the formalism of
problems defined under the reward hypothesis, which sees each task as the maximization of a signal
called reward \cite{Bowling2022-li, Silver2021-uj}. Under this umbrella of problem formulations
lies reinforcement learning, \textit{abbrv.}~RL \cite{Sutton1998-ow}. It is used to
formalize problems of sequential decision making under uncertainty, and fits the problem of
locomotion control well if we see the gait problem as consisting in making a series of reactive
decisions consecutively in the interactive environment that is our simulator. We see immediately
that the RL agent is limited by the speed at which the simulator responds, which is why this
aspect is so critical. The choice of simulator will depend on the trade-off we want to strike.
Be it a prioritization of fidelity or of speed or execution, this choice does not change the task.
For the task to be fully defined however, the agent needs a reward or cost response upon
interaction with the system. This value should serve as guidance, informing the agent seeking
to maximize its cumulative reward ---or equivalently minimize its cumulative cost--- about how
well it is progressing toward the completion of the task.
This signal is designed \cite{Singh2009-cm, Randlov1998-qo, Russell1998-yk},
and can very easily misshapen, leading to unforeseen effects or even exploits
\cite{Amodei2016-vg, Everitt2017-ql, Pan2022-ja, Langosco2022-eo, Skalse2022-xe}.
It is natural to think about energy parsimony when it comes to the primary signal optimized by
human when walking or running. This has been proved against in the case of horses in the past
however \cite{Hoyt1981-ma, Farley1991-fd}.
Nevertheless, it has recently been found that minimizing energy consumption leads
to the emergence of gaits in legged robots \cite{Fu2021-hk}.
A very recent study documents that how agents can overfit to certain reward designs,
and emphasizes that one should construct a reward while keeping in mind that it is the 
reward accumulation that we want the agent to optimize toward \cite{Booth2023-ua}.

Hello

\section{Desiderata}
\label{s:desiderata}

Ultimately what we want is to learn high-performance \footnote{Note, the intricacies of the
performance objective depend on the specific task formulation at hand.}
\textit{in silico} behavior in the shortest
time possible, where the \textit{time} is question encompasses
\textit{(i)} the wall-clock time it takes for the learning agent to solve the task, and
\textit{(ii)} the number of man-hours it takes for a given research idea to be validated
or invalidated.
Of course, those two are related: the shorter an experiment takes, the sooner the practitioner can
iterate on the obtained results and run the next experiments with the adapted code.
But those are two different axes on which the practitioner can make progress for the pipeline to
be more efficient in time.
To have the lowest time possible, our system should involve a simulator that is as fast as possible
and a learning algorithm that requires the fewest possible number of interaction with the simulator
to learn the optimal behavior for the task.

We will treat those in reverse order, first talking
about the latter (a desideratum called \textit{sample efficiency}) in \ref{ss:efficiency},
before dealing with the former (the speed of execution) in \ref{ss:speed}.

\subsection{Sample Efficiency}
\label{ss:efficiency}

TODO

\subsection{Execution Speed}
\label{ss:speed}

TODO

\section{Best Candidate}
\label{s:best}

TODO

% References
\bibliographystyle{plain}
\bibliography{my_bib} % (without the .bib extension)

\end{document}
