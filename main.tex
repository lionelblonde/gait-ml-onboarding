\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}   % For including images
\usepackage{amsmath}    % For mathematical equations
\usepackage{amssymb}    % For mathematical symbols
\usepackage{hyperref}   % For hyperlinks in the document
\usepackage{geometry}   % For setting page margins
\geometry{margin=1in}
\usepackage{cite}       % For managing citations
\usepackage{comment}    % For comment blocks

% Document Information
\title{\textsc{Gait} Project $n+1$}
\author{Lionel Blond\'e}
% \date{\today}

\begin{document}

% Title Page
\maketitle

% Table of Contents
\tableofcontents

% Introduction
\section{Introduction}
\label{s:intro}

In this document, we prioritize conciseness.
The objective is to lay out the various options in have in addressing what we set out to tackle
in this $n+1$ iteration of the \textsc{Gait} project.

To have a detailed account of \textit{what} the project sets out to tackle, please refer to the
proposal, which goes into the motivations and plans in depth.
In here, we lay out the variety of options we have to address the raised research questions.
This exhibition entails general approaches and methods, but also elaborates on the technical
solutions.

The objective is to have a clear picture not only of the research landscape in the area
but also to surface the conceptual and technical challenges that are inherent to the area.
While the \textit{area} in question might have been phrased as ``gait modeling'' in previous
iterations, in this $n+1$ iteration, we formulate is as
\emph{learning locomotion controllers}.

The task of learning how a control policy that knows how to walk \textit{when embodied physically
in the real world} is challenging for a multiplicity of reasons
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}.

We do not aim for embodied deployment.
What we want however is to learn controllers that are able to solve complex locomotion tasks
\textit{in silico}, \textit{i.e.}~in simulation.
While we wish to make use of such learned behaviors to derive insights and guide decisions in real
world tasks (such as surgical assistance), we do not aim to embody these in live robots.

An important point to make however is that most if not all the works in
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}
make use of a simulated world in order to devise the controllers that they deploy in the real
world. In that sense, the research progress carried out during this project could therefore be
transferable and readily extended to research endeavors targeted at embodying gait controllers
in hardware. The difficulty in the deploying such behavior is in closing the gap between the
simulated and real world: the ``sim-to-real'' gap. Despite not having to deal with this gap due
to the lack of deployment need \text{per se}, we face the gap when we have to deal with patient
data. Indeed, data recorded by physicians (\textit{e.g.} motion capture markers) do not live in
the same space as the locomotion simulators at our disposal. Using such data to learn controllers
in simulation required an alignment system that either
\textit{(i)} embeds the markers in the simulation space, or
\textit{(ii)} morphs the simulation space to the world the markers live in.
The latter corresponds to the real world.
Going for option \textit{(ii)} was the goal of iteration $n$ of the \textsc{Gait} project.
This alignment of the simulator with the patient's morphology was lead by the \textsc{BioRob}
team at EPFL. The actual technical solution changed over the course of the project as team
members were coming and going. In the end, the simulation suite that allowed for the flexibily
needed to close the gap between patient data and \textit{in silico} world was
OpenSim \cite{Seth2011-ru}. It stands as a viable option.

Going back a few steps, consider the task of learning a locomotion controller in simulation.
In broad lines, the task requires the researcher practitioner to
\textit{(a)} choose or design a \emph{locomotion simulator}
\textit{(b)} choose or design a \emph{learning signal}
\textit{(c)} choose or design a \emph{learning approach}
\textit{(d)} choose or design a \emph{learning algorithm}
These axes or choice or design are themselves multi-faceted, and this document will attempt to
give the account of the options we have for each.
Note, ``choose'' refers to the selection and use of an already existing solution,
while ``design'' refers to the deliberate creation of a new solution.
As it stands, such a system is therefore highly modular.
Progress can be made on any combination of the four axes above.
How these axes are formulated is flawed in the sense that these axes are not independent from
each other. For example, the approach chosen determines to an extent the classes of algorithms,
or trait thereof, that one might opt for.
Still, it provides a salient language for us to navigate how to tackle the many-sided task. 

Due to the interactions between these axes, the remainder will not catalog the four axes one by
one, giving suggestions for each independently. Instead, we will consider the various learning
approaches that can be considered to solve the task, and expand from there as we go with options.
Organizing the exhibition as such hopefully gives a better account of those options compared to
a catalog with bi-directional links all over the place.

\section{Landscape}
\label{s:landscape}

By \textit{simulator}, we refer to a piece of software that our learning agent interact with in
the course of its learning process. A simulator is what facilitates a \emph{simulation}, which
Sheldon M. Ross defines in \cite{Ross2022-ra} as the imitation of the operation of a real-world
process or system over time. In our case, the process we are interested in is the human gait.
Formally, the simulator is modelled as a Markov Decision Process or MDP \cite{Puterman1994-pf}.
The latter consists of a tuple containing a state space (the sitations the agent finds itself in),
an action space (the decisions the agent might make), a dynamics model that governs the physics of
the world in which the agent interacts, and a reward process. MDPs are used in the formalism of
problems defined under the reward hypothesis, which sees each task as the maximization of a signal
called reward \cite{Bowling2022-li, Silver2021-uj}. Under this umbrella of problem formulations
lies reinforcement learning, \textit{abbrv.}~RL \cite{Sutton1998-ow}. It is used to
formalize problems of sequential decision making under uncertainty, and fits the problem of
locomotion control well if we see the gait problem as consisting in making a series of reactive
decisions consecutively in the interactive environment that is our simulator. We see immediately
that the RL agent is limited by the speed at which the simulator responds, which is why this
aspect is so critical. The choice of simulator will depend on the trade-off we want to strike.
Be it a prioritization of fidelity or of speed or execution, this choice does not change the task.
For the task to be fully defined however, the agent needs a reward or cost response upon
interaction with the system. This value should serve as guidance, informing the agent seeking
to maximize its cumulative reward ---or equivalently minimize its cumulative cost--- about how
well it is progressing toward the completion of the task.
This signal is designed \cite{Singh2009-cm, Randlov1998-qo, Russell1998-yk, Sims1994-zb},
and can very easily misshapen, leading to unforeseen effects or even exploits
\cite{Amodei2016-vg, Everitt2017-ql, Pan2022-ja, Langosco2022-eo, Skalse2022-xe, Sims1994-zb}
It is natural to think about energy parsimony when it comes to the primary signal optimized by
human when walking or running. This has been proved against in the case of horses in the past
however \cite{Hoyt1981-ma, Farley1991-fd}.
\footnote{Particularly interesting quote from \cite{Sims1994-zb}:
``It is important that the physical simulation be reasonably accurate when optimizing
for creatures that can move within it. Any bugs that allow energy leaks from non-conservation,
or even round-off errors, will inevitably be discovered and exploited by the evolving
creatures. Although this can be a lazy and often amusing approach for debugging a physical
modeling system, it is not necessarily the most practical.''}
Nevertheless, it has recently been found that minimizing energy consumption leads
to the emergence of gaits in legged robots \cite{Fu2021-hk}.
A very recent study documents that how agents can overfit to certain reward designs,
and emphasizes that one should construct a reward while keeping in mind that it is the 
reward accumulation that we want the agent to optimize toward \cite{Booth2023-ua}.

Simulators designed for control and locomotion are in general released with pseudo-benchmarked
reward function. These are the result of tedious hand-engineering, often tainted by the issues
raised and studied in \cite{Booth2023-ua}. They are usually aligned with the forward progress
of the agent in the sagittal plane \cite{Schulman2015-jt}. Another way to design a reward that
would incentivize the learning agent to adopt the desired behavior is to leverage the availability
of an expert at the task. This expert could be human or artificial, \textit{e.g.} a hard-coded
controller. This setting is called imitation learning, or learning from demonstration
\cite{Billard2008-jb, Atkeson1997-db, Bagnell2015-ni}.
Assuming alignment of expert and agent gaits, a sensible metric to use as cost could be the
average displacement between the reference expert gait.
Such a cost would be zero when the agent perfectly overlaps with the expert behavior.
The alignment assumption however becomes however increasingly in valid as we stray from purely
synthetic scenarios.

The locomotion controller we want to learn, denoted as $\pi$, maps the state space $\mathcal{S}$
onto the action space $\mathcal{A}$.
If demonstrations are state-action pairs, the problem of learning such a $\pi$ reduces to solving
a supervised learning problem (dubbed \textit{behavioral cloning}).

Note, learning a model with behavioral cloning (BC)
\cite{Pomerleau1989-nh, Pomerleau1990-lm, Ratliff2007-fc}
can be done \textit{offline}, \textit{i.e.}
without interacting with the simulator ---characterizing \textit{online} systems. BC is flawed
in more than one way \cite{Ross2010-eb, Ross2011-dn}.
To learn a viable policy, it requires diverse and action-annotated demonstrations in abundance.
Since this requirement is seldom met, practitioners often turn to apprenticeship learning (AL),
working in lower data regimes, and at least equally importantly, not requiring the expert data
to be annotated with controls (actions).
Indeed, the controls are rarely readily available (\textit{e.g.} video demonstrations).
AL \cite{Abbeel2004-rb, Ratliff2006-kj, Kolter2008-mr} is traditionally consisting
of two stages: 
\textit{(i)} inverse reinforcement learning (IRL; to derive a cost from the expert behavior), and
\textit{(ii)} reinforcement learning (RL; to derive a behavior from the cost being learned).
These stages are sometimes done once, one after the other, or in alternation throughout the entire
learning process \cite{Ho2016-xn}.

Trying to recover the \textit{real} reward (the one optimized by the expert and that led to its
behavior) has been shown to be an ill-posed problem \cite{Ziebart2008-fe}.
A relaxation of AL consisting in learning only a \textit{surrogate} cost has proved enough,
and has led to state-of-the-art results with
generative adversarial imitation learning (GAIL) \cite{Ho2016-bv}.
Being a relaxation of GAIL, it can learned from control-annotated demonstrations, or from
observations (states) alone, without actions from the expert.
The first setting is called ``ILfD'', the second ``ILfO'', for ``Imitation Learning from
Observations'' \cite{Merel2017-lo, Liu2017-dr, Reed2018-ga, Fu2018-zu, Aytar2018-by,
Torabi2018-fg, Torabi2018-nb}.
Adversarial imitation learning is the go-to way to learn a sensible cost (reward) signal,
but what about step \textit{(ii)} above ---the RL method to use in order to learn
a controller from the surrogate reward?

Prime candidates to solve the RL outer loop and that have proved successful in various scenarios
including locomotion tasks are trust-region methods. The figure-heads of this family of algorithms
are TRPO \cite{}, PPO \cite{},
MPO \cite{}, V-MPO \cite{},
and MDPO \cite{}.
The latter, which stands for ``Mirror Descent Policy Optimization'' has had its first surge of use
recently in an LLM context \cite{}.
MDPO is, according to the original paper, simpler and more stable than PPO, which is infamously
tedious to tame \cite{}.
MDPO has been introduced in two variants, one stems from PPO, the other from SAC \cite{},
a method that falls under an architecture called \textit{actor-critic}.
While PPO is \textit{on-policy}, SAC (which stands for ``Soft Actor-Critic''),
is \textit{off-policy}.
A method is said to be on-policy when the policy being learned and the one being used are one and
the same. When they are distinct, the method is off-policy (and the two policies are called the
\textit{behavior} and the \textit{target} policies).
We will shortly tackle why off-policy-ness is of great interest when dealing with real-world RL
in scenarios where safety is critical and where data scarcity is being faced
(\textit{cf.}~\ref{ss:efficiency}).

\begin{comment}
another way if sequential data: sequence modelling
    task: next token prediction with transformer
    task: RL or IL, but with policy and/or value that exacts features with transformer
    RT1-2, Q-Transformer, Diffusion policy, BeT
\end{comment}

\section{Desiderata}
\label{s:desiderata}

Ultimately what we want is to learn high-performance \footnote{Note, the intricacies of the
performance objective depend on the specific task formulation at hand.}
\textit{in silico} behavior in the shortest
time possible, where the \textit{time} is question encompasses
\textit{(i)} the wall-clock time it takes for the learning agent to solve the task, and
\textit{(ii)} the number of man-hours it takes for a given research idea to be validated
or invalidated.
Of course, those two are related: the shorter an experiment takes, the sooner the practitioner can
iterate on the obtained results and run the next experiments with the adapted code.
But those are two different axes on which the practitioner can make progress for the pipeline to
be more efficient in time.
To have the lowest time possible, our system should involve a simulator that is as fast as possible
and a learning algorithm that requires the fewest possible number of interaction with the simulator
to learn the optimal behavior for the task.

We will treat those in reverse order, first talking
about the latter (a desideratum called \textit{sample efficiency}) in \ref{ss:efficiency},
before dealing with the former (the speed of execution) in \ref{ss:speed}.

\subsection{Sample Efficiency}
\label{ss:efficiency}

TODO

\begin{comment}
sample-efficiency very important for the project
two main avenues for potential major sample-efficiency improvements
1. off-policy learning
2. model-based learning
    forward model, world model (more loose, less strict signature)
    dyna style, SVG style, or MPC style, i.e. optimal control (book reference)
\end{comment}

\subsection{Execution Speed}
\label{ss:speed}

TODO

\begin{comment}

\end{comment}

\section{Best Candidate}
\label{s:best}

TODO

\begin{comment}
====extra things====
in-context learning (in-context RL --ICRL)
trust-region methods (TRPO, PPO, MDPO)
TD-MPC2 >> DreamerV3 for continuous control
Temporal Cycle-Consistency Learning (TCC)
\end{comment}

% References
\bibliographystyle{plain}
\bibliography{my_bib} % (without the .bib extension)

\end{document}
