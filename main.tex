\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}   % For including images
\usepackage{amsmath}    % For mathematical equations
\usepackage{amssymb}    % For mathematical symbols
\usepackage{hyperref}   % For hyperlinks in the document
\usepackage{geometry}   % For setting page margins
\geometry{margin=1in}
\usepackage{cite}       % For managing citations
\usepackage{comment}    % For comment blocks

% Document Information
\title{\textsc{Gait} Project $n+1$}
\author{Lionel Blond\'e}
% \date{\today}

\begin{document}

% Title Page
\maketitle

% Table of Contents
\tableofcontents

% Introduction
\section{Introduction}
\label{s:intro}

In this document, we prioritize conciseness.
The objective is to lay out the various options in have in addressing what we set out to tackle
in this $n+1$ iteration of the \textsc{Gait} project.

To have a detailed account of \textit{what} the project sets out to tackle, please refer to the
proposal, which goes into the motivations and plans in depth.
In here, we lay out the variety of options we have to address the raised research questions.
This exhibition entails general approaches and methods, but also elaborates on the technical
solutions.

The objective is to have a clear picture not only of the research landscape in the area
but also to surface the conceptual and technical challenges that are inherent to the area.
While the \textit{area} in question might have been phrased as ``gait modeling'' in previous
iterations, in this $n+1$ iteration, we formulate is as
\emph{learning locomotion controllers}.

The task of learning how a control policy that knows how to walk \textit{when embodied physically
in the real world} is challenging for a multiplicity of reasons
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}.

We do not aim for embodied deployment.
What we want however is to learn controllers that are able to solve complex locomotion tasks
\textit{in silico}, \textit{i.e.}~in simulation.
While we wish to make use of such learned behaviors to derive insights and guide decisions in real
world tasks (such as surgical assistance), we do not aim to embody these in live robots.

An important point to make however is that most if not all the works in
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}
make use of a simulated world in order to devise the controllers that they deploy in the real
world. In that sense, the research progress carried out during this project could therefore be
transferable and readily extended to research endeavors targeted at embodying gait controllers
in hardware. The difficulty in the deploying such behavior is in closing the gap between the
simulated and real world: the ``sim-to-real'' gap. Despite not having to deal with this gap due
to the lack of deployment need \text{per se}, we face the gap when we have to deal with patient
data. Indeed, data recorded by physicians (\textit{e.g.} motion capture markers) do not live in
the same space as the locomotion simulators at our disposal. Using such data to learn controllers
in simulation required an alignment system that either
\textit{(i)} embeds the markers in the simulation space, or
\textit{(ii)} morphs the simulation space to the world the markers live in.
The latter corresponds to the real world.
Going for option \textit{(ii)} was the goal of iteration $n$ of the \textsc{Gait} project.
This alignment of the simulator with the patient's morphology was lead by the \textsc{BioRob}
team at EPFL. The actual technical solution changed over the course of the project as team
members were coming and going. In the end, the simulation suite that allowed for the flexibily
needed to close the gap between patient data and \textit{in silico} world was
OpenSim \cite{Seth2011-ru}. It stands as a viable option.

Going back a few steps, consider the task of learning a locomotion controller in simulation.
In broad lines, the task requires the researcher practitioner to
\textit{(a)} choose or design a \emph{locomotion simulator}
\textit{(b)} choose or design a \emph{learning signal}
\textit{(c)} choose or design a \emph{learning approach}
\textit{(d)} choose or design a \emph{learning algorithm}
These axes or choice or design are themselves multi-faceted, and this document will attempt to
give the account of the options we have for each.
Note, ``choose'' refers to the selection and use of an already existing solution,
while ``design'' refers to the deliberate creation of a new solution.
As it stands, such a system is therefore highly modular.
Progress can be made on any combination of the four axes above.
How these axes are formulated is flawed in the sense that these axes are not independent from
each other. For example, the approach chosen determines to an extent the classes of algorithms,
or trait thereof, that one might opt for.
Still, it provides a salient language for us to navigate how to tackle the many-sided task. 

Due to the interactions between these axes, the remainder will not catalog the four axes one by
one, giving suggestions for each independently. Instead, we will consider the various learning
approaches that can be considered to solve the task, and expand from there as we go with options.
Organizing the exhibition as such hopefully gives a better account of those options compared to
a catalog with bi-directional links all over the place.

\section{Landscape}
\label{s:landscape}

By \textit{simulator}, we refer to a piece of software that our learning agent interact with in
the course of its learning process. A simulator is what facilitates a \emph{simulation}, which
Sheldon M. Ross defines in \cite{Ross2022-ra} as the imitation of the operation of a real-world
process or system over time. In our case, the process we are interested in is the human gait.
Formally, the simulator is modelled as a Markov Decision Process or MDP \cite{Puterman1994-pf}.
The latter consists of a tuple containing a state space (the sitations the agent finds itself in),
an action space (the decisions the agent might make), a dynamics model that governs the physics of
the world in which the agent interacts, and a reward process. MDPs are used in the formalism of
problems defined under the reward hypothesis, which sees each task as the maximization of a signal
called reward \cite{Bowling2022-li, Silver2021-uj}. Under this umbrella of problem formulations
lies reinforcement learning, \textit{abbrv.}~RL \cite{Sutton1998-ow}. It is used to
formalize problems of sequential decision making under uncertainty, and fits the problem of
locomotion control well if we see the gait problem as consisting in making a series of reactive
decisions consecutively in the interactive environment that is our simulator. We see immediately
that the RL agent is limited by the speed at which the simulator responds, which is why this
aspect is so critical. The choice of simulator will depend on the trade-off we want to strike.
Be it a prioritization of fidelity or of speed or execution, this choice does not change the task.
For the task to be fully defined however, the agent needs a reward or cost response upon
interaction with the system. This value should serve as guidance, informing the agent seeking
to maximize its cumulative reward ---or equivalently minimize its cumulative cost--- about how
well it is progressing toward the completion of the task.
This signal is designed \cite{Singh2009-cm, Randlov1998-qo, Russell1998-yk, Sims1994-zb},
and can very easily misshapen, leading to unforeseen effects or even exploits
\cite{Amodei2016-vg, Everitt2017-ql, Pan2022-ja, Langosco2022-eo, Skalse2022-xe, Sims1994-zb}
It is natural to think about energy parsimony when it comes to the primary signal optimized by
human when walking or running. This has been proved against in the case of horses in the past
however \cite{Hoyt1981-ma, Farley1991-fd}.
\footnote{Particularly interesting quote from \cite{Sims1994-zb}:
``It is important that the physical simulation be reasonably accurate when optimizing
for creatures that can move within it. Any bugs that allow energy leaks from non-conservation,
or even round-off errors, will inevitably be discovered and exploited by the evolving
creatures. Although this can be a lazy and often amusing approach for debugging a physical
modeling system, it is not necessarily the most practical.''}
Nevertheless, it has recently been found that minimizing energy consumption leads
to the emergence of gaits in legged robots \cite{Fu2021-hk}.
A very recent study documents that how agents can overfit to certain reward designs,
and emphasizes that one should construct a reward while keeping in mind that it is the 
reward accumulation that we want the agent to optimize toward \cite{Booth2023-ua}.

Simulators designed for control and locomotion are in general released with pseudo-benchmarked
reward function. These are the result of tedious hand-engineering, often tainted by the issues
raised and studied in \cite{Booth2023-ua}. They are usually aligned with the forward progress
of the agent in the sagittal plane \cite{Schulman2015-jt}. Another way to design a reward that
would incentivize the learning agent to adopt the desired behavior is to leverage the availability
of an expert at the task. This expert could be human or artificial, \textit{e.g.} a hard-coded
controller. This setting is called imitation learning, or learning from demonstration
\cite{Billard2008-jb, Atkeson1997-db, Bagnell2015-ni}.
Assuming alignment of expert and agent gaits, a sensible metric to use as cost could be the
average displacement between the reference expert gait.
Such a cost would be zero when the agent perfectly overlaps with the expert behavior.
The alignment assumption however becomes however increasingly in valid as we stray from purely
synthetic scenarios.

The locomotion controller we want to learn, denoted as $\pi$, maps the state space $\mathcal{S}$
onto the action space $\mathcal{A}$.
If demonstrations are state-action pairs, the problem of learning such a $\pi$ reduces to solving
a supervised learning problem (dubbed \textit{behavioral cloning}).

Note, learning a model with behavioral cloning (BC)
\cite{Pomerleau1989-nh, Pomerleau1990-lm, Ratliff2007-fc}
can be done \textit{offline}, \textit{i.e.}
without interacting with the simulator ---characterizing \textit{online} systems. BC is flawed
in more than one way \cite{Ross2010-eb, Ross2011-dn}.
To learn a viable policy, it requires diverse and action-annotated demonstrations in abundance.
Since this requirement is seldom met, practitioners often turn to apprenticeship learning (AL),
working in lower data regimes, and at least equally importantly, not requiring the expert data
to be annotated with controls (actions).
Indeed, the controls are rarely readily available (\textit{e.g.} video demonstrations).
AL \cite{Abbeel2004-rb, Ratliff2006-kj, Kolter2008-mr} is traditionally consisting
of two stages: 
\textit{(i)} inverse reinforcement learning (IRL; to derive a cost from the expert behavior), and
\textit{(ii)} reinforcement learning (RL; to derive a behavior from the cost being learned).
These stages are sometimes done once, one after the other, or in alternation throughout the entire
learning process \cite{Ho2016-xn}.

Trying to recover the \textit{real} reward (the one optimized by the expert and that led to its
behavior) has been shown to be an ill-posed problem \cite{Ziebart2008-fe}.
A relaxation of AL consisting in learning only a \textit{surrogate} cost has proved enough,
and has led to state-of-the-art results with
generative adversarial imitation learning (GAIL) \cite{Ho2016-bv}.
Being a relaxation of GAIL, it can learned from control-annotated demonstrations, or from
observations (states) alone, without actions from the expert.
The first setting is called ``ILfD'', the second ``ILfO'', for ``Imitation Learning from
Observations'' \cite{Merel2017-lo, Liu2017-dr, Reed2018-ga, Fu2018-zu, Aytar2018-by,
Torabi2018-fg, Torabi2018-nb}.
Adversarial imitation learning is the go-to way to learn a sensible cost (reward) signal,
but what about step \textit{(ii)} above ---the RL method to use in order to learn
a controller from the surrogate reward?

Prime candidates to solve the RL outer loop and that have proved successful in various scenarios
including locomotion tasks are \emph{trust-region methods}.
The figure-heads of this family of algorithms
are TRPO \cite{Schulman2015-jt}, PPO \cite{Schulman2017-ou},
MPO \cite{Abdolmaleki2018-sp}, V-MPO \cite{Francis_Song2020-sd},
and MDPO \cite{Tomar2020-ts}.

The latter, which stands for ``Mirror Descent Policy Optimization'' has had its first surge of use
recently in an LLM context \cite{Gunter2024-hu}.
MDPO is, according to the original paper, simpler and more stable than PPO, which is infamously
tedious to tame \cite{Engstrom2019-jt, Huang2022-bv, Moalla2024-xq}.
MDPO has been introduced in two variants, one stems from PPO,
the other from SAC \cite{Haarnoja2018-bm, Haarnoja2018-uo},
a method that falls under an architecture called \textit{actor-critic}
\cite{Crites1995-hn, Konda2000-ef, Sutton2001-vh}.
While PPO is \textit{on-policy}, SAC (which stands for ``Soft Actor-Critic''),
is \textit{off-policy}.
A method is said to be on-policy when the policy being learned and the one being used are one and
the same. When they are distinct, the method is off-policy (and the two policies are called the
\textit{behavior} and the \textit{target} policies).
We will shortly tackle why off-policy-ness is of great interest when dealing with real-world RL
in scenarios where safety is critical and where data scarcity is being faced
(\textit{cf.}~\ref{ss:efficiency}).

If one has access to demonstrations in abundance \emph{and} organized in complete sequences 
(\textit{i.e.} not sub-sampled)
then one can turn to \emph{sequence modeling}
(\textit{cf.}~\cite{Goodfellow2016-ev}, \textsc{Chap.}~10).
Tackling control problem as sequence prediction is not new, but is disregards the fact that
control agents, as interactive in nature, must comply with the physics of their environment
\cite{Salzmann2020-jl}. Typical technical solutions have involved RNN/LSTM backbones for
their ability to propagate information in time across timesteps.
It does so by compressing previous information seen in the trajectory (accumulating over time)
into a fixed size hidden state.

Mainstream attention has moved onto a different architecture,
the Transformer \cite{Vaswani2017-lk},
which splits sequences in tokens from a fixed vocabulary,
and learning how to build sequences by trying to predict the next token given all the previous
ones in the sequence.
The core mechanism at play is \emph{attention}, and specifically \emph{causal attention}, where
the future is masked: the model can not attend to future token in its prediction.
This ensures temporal consistency.
The two concurrent works that first tackled control problems
\textit{(i)} as sequence modeling problems,
\textit{(ii)} with transformers, are
\cite{Chen2021-qq} and \cite{Janner2021-wn}.
An vast array of works has spawned from these.
Still, they are inherently flawed because they are not trained online, but from an offline dataset.
Despite this flaw, they have proved very effective for real-world humanoid locomotion tasks
\cite{Radosavovic2024-xl}
and real-world robotics tasks \cite{Brohan2022-vs, Brohan2023-rm}.
The obvious caveat is that it is a challenging engineering endeavor.
Hybrid solutions have led to the best results so far.
These leverage the power of transformers as feature extractors from sequences, but the task is of
the RL type, \textit{i.e.} maximize the cumulative reward, or IL type.
Humanoid locomotion via RL is the goal of \cite{Radosavovic2024-dm},
with broader robotics tasks via RL being the focus in the Q-Transformer \cite{Chebotar2023-hq},
and via BC (IL) in Behavior Transformer (BeT) \cite{Shafiullah2022-ux}.
Diffusion heads have also been used on top of transformer backbones with great practical success,
like in the Diffusion Policy \cite{Chi2023-is}.

\section{Desiderata}
\label{s:desiderata}

Ultimately what we want is to learn high-performance \footnote{Note, the intricacies of the
performance objective depend on the specific task formulation at hand.}
\textit{in silico} behavior in the shortest
time possible, where the \textit{time} is question encompasses
\textit{(i)} the wall-clock time it takes for the learning agent to solve the task, and
\textit{(ii)} the number of man-hours it takes for a given research idea to be validated
or invalidated.
Of course, those two are related: the shorter an experiment takes, the sooner the practitioner can
iterate on the obtained results and run the next experiments with the adapted code.
But those are two different axes on which the practitioner can make progress for the pipeline to
be more efficient in time.
To have the lowest time possible, our system should involve a simulator that is as fast as possible
and a learning algorithm that requires the fewest possible number of interaction with the simulator
to learn the optimal behavior for the task.

We will treat those in reverse order, first talking
about the latter (a desideratum called \textit{sample efficiency}) in \ref{ss:efficiency},
before dealing with the former (the speed of execution) in \ref{ss:speed}.

\subsection{Sample Efficiency}
\label{ss:efficiency}

TODO

\begin{comment}
sample-efficiency very important for the project
two main avenues for potential major sample-efficiency improvements
1. off-policy learning
2. model-based learning
    forward model, world model (more loose, less strict signature)
    Dyna style, SVG style, or MPC style, i.e. optimal control (book reference)
SVG style methods: BP (BPTT) through env possible because world modelled with a NN
but there is another way to BP through env without learning an extra model: using a diffsim
diffsim are therefore a way to benefit from the sample efficiency of SVG-style model-based methods
    without the need to learn anything else than the agent's policy (and value)
there is a clear advantage with diffsim: the world is not learned from data points
    which we usually don't have before and it is the agent that collects data while learning
    instead, it is crafted by the practitioner who injects acquired domain knowledge
    and structural biases
    the diffsim designer has knowledge about the domain and knows how the design choices impact
    the interaction time (directly a consequence of the integration time to dynamics to kinematics)
    the designer can strike the desired trade-offs between fidelity and speed: as the fidelity
    increases, more sophisticated models are used, which are more costly to integrate over.
diffsim are hard-coded and therefore not prone to model hacking (model exploitation/bias) like the
    learned models are in model-based RL
locomotion sim are tedious to make diff notably because of the contacts, which introduce
    discontinuities in the sim response and therefore Dirac deltas in the diffsim gradients
also, in order to BP from the RL objective through the diffsim backward long trajectories, the
    reward itself need be a diff signal, which is not necessarily obvious in common suites
    in the case of IL, displacement error need only the dynamics to be diff, but diffsims
last note: diffsim are challenging to craft, and trade-offs / sacrifices are not rare
    therefore, straying from the document use cases with original or novel usage often comes with
    surprises such as numerical bugs
\end{comment}

\subsection{Execution Speed}
\label{ss:speed}

TODO

\begin{comment}
domain randomization very useful to be able to communicate to the agent what is relevant and what
    is not; by seeing a variety of textures and colors that do not change the task in nature, we
    want the model to understand what it should be invariant to (cf. literature on self-supervised
    learning)
    SO, if one want to close the gap, showing lots of things to the agent is crucial, and for that
    to be feasible, the sim need be fast

problem of character animation in rendering engine is almost identical
    an advantage they have is that game engines do not car as much about contacts being sharp
    usage of dampeners that make contact smooth --making them technically infringe physics
    those are usually very fast and done by game studios (so run on GPU), but clear trade-off

loco sims that are high fidelity and well-vetted are all CPU-based
    GPU-based sims are considerably faster but some operations in locomotion model primitives
    can not be done via big matrix multiplications and are therefore not able to leverage the
    capabilities of GPU, which is why those sims have not been ported to GPU as of yet
    BRAX/MLX might be getting there very soon, so it is very important to keep an eye on those
    dlex from Nvidia, warp
    again: the more we go high-fidelity with real world, the better we can close the sim2real gap

muscles
    OpenSim: CPU
    dflex: GPU
\end{comment}

\section{Best Candidate}
\label{s:best}

TODO

\begin{comment}
====extra things====


TD-MPC2 >> DreamerV3 for continuous control (DreamerV3 better for discrete control,
    although allegedly worse now than IRIS, Diamond, TWM, etc.)
this requires to learn a world model, to then plan on

MoCap
Temporal Cycle-Consistency Learning (TCC)
in-context learning (in-context RL --ICRL)
\end{comment}

% References
\bibliographystyle{plain}
\bibliography{my_bib} % (without the .bib extension)

\end{document}
