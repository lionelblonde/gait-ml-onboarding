\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}   % For including images
\usepackage{amsmath}    % For mathematical equations
\usepackage{amssymb}    % For mathematical symbols
\usepackage{hyperref}   % For hyperlinks in the document
\usepackage{geometry}   % For setting page margins
\geometry{margin=1in}
\usepackage{cite}       % For managing citations
\usepackage{comment}    % For comment blocks

% Document Information
\title{\textsc{Gait} Project $n+1$}
\author{Lionel Blond\'e}
% \date{\today}

\begin{document}

% Title Page
\maketitle

% Table of Contents
\tableofcontents

% Introduction
\section{Introduction}
\label{s:intro}

In this document, we prioritize conciseness.
The objective is to lay out the various options in have in addressing what we set out to tackle
in this $n+1$ iteration of the \textsc{Gait} project.

To have a detailed account of \textit{what} the project sets out to tackle, please refer to the
proposal, which goes into the motivations and plans in depth.
In here, we lay out the variety of options we have to address the raised research questions.
This exhibition entails general approaches and methods, but also elaborates on the technical
solutions.

The objective is to have a clear picture not only of the research landscape in the area
but also to surface the conceptual and technical challenges that are inherent to the area.
While the \textit{area} in question might have been phrased as ``gait modeling'' in previous
iterations, in this $n+1$ iteration, we formulate is as
\emph{learning locomotion controllers}.

The task of learning how a control policy that knows how to walk \textit{when embodied physically
in the real world} is challenging for a multiplicity of reasons
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}.

We do not aim for embodied deployment.
What we want however is to learn controllers that are able to solve complex locomotion tasks
\textit{in silico}, \textit{i.e.}~in simulation.
While we wish to make use of such learned behaviors to derive insights and guide decisions in real
world tasks (such as surgical assistance), we do not aim to embody these in live robots.

An important point to make however is that most if not all the works in
\cite{Radosavovic2024-dm, Caluwaerts2023-ko, Yang2023-jr, Kaufmann2023-yc, Smith2022-sm}
make use of a simulated world in order to devise the controllers that they deploy in the real
world. In that sense, the research progress carried out during this project could therefore be
transferable and readily extended to research endeavors targeted at embodying gait controllers
in hardware. The difficulty in the deploying such behavior is in closing the gap between the
simulated and real world: the ``sim-to-real'' gap. Despite not having to deal with this gap due
to the lack of deployment need \text{per se}, we face the gap when we have to deal with patient
data. Indeed, data recorded by physicians (\textit{e.g.} motion capture markers) do not live in
the same space as the locomotion simulators at our disposal. Using such data to learn controllers
in simulation required an alignment system that either
\textit{(i)} embeds the markers in the simulation space, or
\textit{(ii)} morphs the simulation space to the world the markers live in.
The latter corresponds to the real world.
Going for option \textit{(ii)} was the goal of iteration $n$ of the \textsc{Gait} project.
This alignment of the simulator with the patient's morphology was lead by the \textsc{BioRob}
team at EPFL. The actual technical solution changed over the course of the project as team
members were coming and going. In the end, the simulation suite that allowed for the flexibily
needed to close the gap between patient data and \textit{in silico} world was
OpenSim \cite{Seth2011-ru}. It stands as a viable option.

Going back a few steps, consider the task of learning a locomotion controller in simulation.
In broad lines, the task requires the researcher practitioner to
\textit{(a)} choose or design a \emph{locomotion simulator}
\textit{(b)} choose or design a \emph{learning signal}
\textit{(c)} choose or design a \emph{learning approach}
\textit{(d)} choose or design a \emph{learning algorithm}
These axes or choice or design are themselves multi-faceted, and this document will attempt to
give the account of the options we have for each.
Note, ``choose'' refers to the selection and use of an already existing solution,
while ``design'' refers to the deliberate creation of a new solution.
As it stands, such a system is therefore highly modular.
Progress can be made on any combination of the four axes above.
How these axes are formulated is flawed in the sense that these axes are not independent from
each other. For example, the approach chosen determines to an extent the classes of algorithms,
or trait thereof, that one might opt for.
Still, it provides a salient language for us to navigate how to tackle the many-sided task. 

Due to the interactions between these axes, the remainder will not catalog the four axes one by
one, giving suggestions for each independently. Instead, we will consider the various learning
approaches that can be considered to solve the task, and expand from there as we go with options.
Organizing the exhibition as such hopefully gives a better account of those options compared to
a catalog with bi-directional links all over the place.

\section{Landscape}
\label{s:landscape}

By \textit{simulator}, we refer to a piece of software that our learning agent interact with in
the course of its learning process. A simulator is what facilitates a \emph{simulation}, which
Sheldon M. Ross defines in \cite{Ross2022-ra} as the imitation of the operation of a real-world
process or system over time. In our case, the process we are interested in is the human gait.
Formally, the simulator is modelled as a Markov Decision Process or MDP \cite{Puterman1994-pf}.
The latter consists of a tuple containing a state space (the sitations the agent finds itself in),
an action space (the decisions the agent might make), a dynamics model that governs the physics of
the world in which the agent interacts, and a reward process. MDPs are used in the formalism of
problems defined under the reward hypothesis, which sees each task as the maximization of a signal
called reward \cite{Bowling2022-li, Silver2021-uj}. Under this umbrella of problem formulations
lies reinforcement learning, \textit{abbrv.}~RL \cite{Sutton1998-ow}. It is used to
formalize problems of sequential decision making under uncertainty, and fits the problem of
locomotion control well if we see the gait problem as consisting in making a series of reactive
decisions consecutively in the interactive environment that is our simulator. We see immediately
that the RL agent is limited by the speed at which the simulator responds, which is why this
aspect is so critical. The choice of simulator will depend on the trade-off we want to strike.
Be it a prioritization of fidelity or of speed or execution, this choice does not change the task.
For the task to be fully defined however, the agent needs a reward or cost response upon
interaction with the system. This value should serve as guidance, informing the agent seeking
to maximize its cumulative reward ---or equivalently minimize its cumulative cost--- about how
well it is progressing toward the completion of the task.
This signal is designed \cite{Singh2009-cm, Randlov1998-qo, Russell1998-yk, Sims1994-zb},
and can very easily misshapen, leading to unforeseen effects or even exploits
\cite{Amodei2016-vg, Everitt2017-ql, Pan2022-ja, Langosco2022-eo, Skalse2022-xe, Sims1994-zb}
It is natural to think about energy parsimony when it comes to the primary signal optimized by
human when walking or running. This has been proved against in the case of horses in the past
however \cite{Hoyt1981-ma, Farley1991-fd}.
\footnote{Particularly interesting quote from \cite{Sims1994-zb}:
``It is important that the physical simulation be reasonably accurate when optimizing
for creatures that can move within it. Any bugs that allow energy leaks from non-conservation,
or even round-off errors, will inevitably be discovered and exploited by the evolving
creatures. Although this can be a lazy and often amusing approach for debugging a physical
modeling system, it is not necessarily the most practical.''}
Nevertheless, it has recently been found that minimizing energy consumption leads
to the emergence of gaits in legged robots \cite{Fu2021-hk}.
A very recent study documents that how agents can overfit to certain reward designs,
and emphasizes that one should construct a reward while keeping in mind that it is the 
reward accumulation that we want the agent to optimize toward \cite{Booth2023-ua}.

Simulators designed for control and locomotion are in general released with pseudo-benchmarked
reward function. These are the result of tedious hand-engineering, often tainted by the issues
raised and studied in \cite{Booth2023-ua}. They are usually aligned with the forward progress
of the agent in the sagittal plane \cite{Schulman2015-jt}. Another way to design a reward that
would incentivize the learning agent to adopt the desired behavior is to leverage the availability
of an expert at the task. This expert could be human or artificial, \textit{e.g.} a hard-coded
controller. This setting is called imitation learning, or learning from demonstration
\cite{Billard2008-jb, Atkeson1997-db, Bagnell2015-ni}.
Assuming alignment of expert and agent gaits, a sensible metric to use as cost could be the
average displacement between the reference expert gait.
Such a cost would be zero when the agent perfectly overlaps with the expert behavior.
The alignment assumption however becomes however increasingly in valid as we stray from purely
synthetic scenarios.

The locomotion controller we want to learn, denoted as $\pi$, maps the state space $\mathcal{S}$
onto the action space $\mathcal{A}$.
If demonstrations are state-action pairs, the problem of learning such a $\pi$ reduces to solving
a supervised learning problem (dubbed \textit{behavioral cloning}).

Note, learning a model with behavioral cloning (BC)
\cite{Pomerleau1989-nh, Pomerleau1990-lm, Ratliff2007-fc}
can be done \textit{offline}, \textit{i.e.}
without interacting with the simulator ---characterizing \textit{online} systems. BC is flawed
in more than one way \cite{Ross2010-eb, Ross2011-dn}.
To learn a viable policy, it requires diverse and action-annotated demonstrations in abundance.
Since this requirement is seldom met, practitioners often turn to apprenticeship learning (AL),
working in lower data regimes, and at least equally importantly, not requiring the expert data
to be annotated with controls (actions).
Indeed, the controls are rarely readily available (\textit{e.g.} video demonstrations).
AL \cite{Abbeel2004-rb, Ratliff2006-kj, Kolter2008-mr} is traditionally consisting
of two stages: 
\textit{(i)} inverse reinforcement learning (IRL; to derive a cost from the expert behavior), and
\textit{(ii)} reinforcement learning (RL; to derive a behavior from the cost being learned).
These stages are sometimes done once, one after the other, or in alternation throughout the entire
learning process \cite{Ho2016-xn}.

Trying to recover the \textit{real} reward (the one optimized by the expert and that led to its
behavior) has been shown to be an ill-posed problem \cite{Ziebart2008-fe}.
A relaxation of AL consisting in learning only a \textit{surrogate} cost has proved enough,
and has led to state-of-the-art results with
generative adversarial imitation learning (GAIL) \cite{Ho2016-bv}.
Being a relaxation of GAIL, it can learned from control-annotated demonstrations, or from
observations (states) alone, without actions from the expert.
The first setting is called ``ILfD'', the second ``ILfO'', for ``Imitation Learning from
Observations'' \cite{Merel2017-lo, Liu2017-dr, Reed2018-ga, Fu2018-zu, Aytar2018-by,
Torabi2018-fg, Torabi2018-nb}.
Adversarial imitation learning is the go-to way to learn a sensible cost (reward) signal,
but what about step \textit{(ii)} above ---the RL method to use in order to learn
a controller from the surrogate reward?

Prime candidates to solve the RL outer loop and that have proved successful in various scenarios
including locomotion tasks are \emph{trust-region methods}.
The figure-heads of this family of algorithms
are TRPO \cite{Schulman2015-jt}, PPO \cite{Schulman2017-ou},
MPO \cite{Abdolmaleki2018-sp}, V-MPO \cite{Francis_Song2020-sd},
and MDPO \cite{Tomar2020-ts}.

The latter, which stands for ``Mirror Descent Policy Optimization'' has had its first surge of use
recently in an LLM context \cite{Gunter2024-hu}.
MDPO is, according to the original paper, simpler and more stable than PPO, which is infamously
tedious to tame \cite{Engstrom2019-jt, Huang2022-bv, Moalla2024-xq}.
MDPO has been introduced in two variants, one stems from PPO,
the other from SAC \cite{Haarnoja2018-bm, Haarnoja2018-uo},
a method that falls under an architecture called \textit{actor-critic}
\cite{Crites1995-hn, Konda2000-ef, Sutton2001-vh}.
While PPO is \textit{on-policy}, SAC (which stands for ``Soft Actor-Critic''),
is \textit{off-policy}.
A method is said to be on-policy when the policy being learned and the one being used are one and
the same. When they are distinct, the method is off-policy (and the two policies are called the
\textit{behavior} and the \textit{target} policies).
We will shortly tackle why off-policy-ness is of great interest when dealing with real-world RL
in scenarios where safety is critical and where data scarcity is being faced
(\textit{cf.}~\ref{ss:efficiency}).

If one has access to demonstrations in abundance \emph{and} organized in complete sequences 
(\textit{i.e.} not sub-sampled)
then one can turn to \emph{sequence modeling}
(\textit{cf.}~\cite{Goodfellow2016-ev}, \textsc{Chap.}~10).
Tackling control problem as sequence prediction is not new, but is disregards the fact that
control agents, as interactive in nature, must comply with the physics of their environment
\cite{Salzmann2020-jl}. Typical technical solutions have involved RNN/LSTM backbones for
their ability to propagate information in time across timesteps.
It does so by compressing previous information seen in the trajectory (accumulating over time)
into a fixed size hidden state.

Mainstream attention has moved onto a different architecture,
the Transformer \cite{Vaswani2017-lk},
which splits sequences in tokens from a fixed vocabulary,
and learning how to build sequences by trying to predict the next token given all the previous
ones in the sequence.
The core mechanism at play is \emph{attention}, and specifically \emph{causal attention}, where
the future is masked: the model can not attend to future token in its prediction.
This ensures temporal consistency.
The two concurrent works that first tackled control problems
\textit{(i)} as sequence modeling problems,
\textit{(ii)} with transformers, are
\cite{Chen2021-qq} and \cite{Janner2021-wn}.
An vast array of works has spawned from these.
Still, they are inherently flawed because they are not trained online, but from an offline dataset.
Despite this flaw, they have proved very effective for real-world humanoid locomotion tasks
\cite{Radosavovic2024-xl}
and real-world robotics tasks \cite{Brohan2022-vs, Brohan2023-rm}.
The obvious caveat is that it is a challenging engineering endeavor.
Hybrid solutions have led to the best results so far.
These leverage the power of transformers as feature extractors from sequences, but the task is of
the RL type, \textit{i.e.} maximize the cumulative reward, or IL type.
Humanoid locomotion via RL is the goal of \cite{Radosavovic2024-dm},
with broader robotics tasks via RL being the focus in the Q-Transformer \cite{Chebotar2023-hq},
and via BC (IL) in Behavior Transformer (BeT) \cite{Shafiullah2022-ux}.
Diffusion heads have also been used on top of transformer backbones with great practical success,
like in the Diffusion Policy \cite{Chi2023-is}.

\section{Desiderata}
\label{s:desiderata}

Ultimately what we want is to learn high-performance \footnote{Note, the intricacies of the
performance objective depend on the specific task formulation at hand.}
\textit{in silico} behavior in the shortest
time possible, where the \textit{time} is question encompasses
\textit{(i)} the wall-clock time it takes for the learning agent to solve the task, and
\textit{(ii)} the number of man-hours it takes for a given research idea to be validated
or invalidated.
Of course, those two are related: the shorter an experiment takes, the sooner the practitioner can
iterate on the obtained results and run the next experiments with the adapted code.
But those are two different axes on which the practitioner can make progress for the pipeline to
be more efficient in time.
To have the lowest time possible, our system should involve a simulator that is as fast as possible
and a learning algorithm that requires the fewest possible number of interaction with the simulator
to learn the optimal behavior for the task.

We will treat those in reverse order, first talking
about the latter (a desideratum called \textit{sample efficiency}) in \ref{ss:efficiency},
before dealing with the former (the speed of execution) in \ref{ss:speed}.

\subsection{Sample Efficiency}
\label{ss:efficiency}

Since simulators can be complex and therefore slow,
it is of primary interest to treat with methods
that require few interactions with the simulator,
\textit{i.e.} sample-efficient methods.

There are two main avenues for potential major improvements in sample efficiency:
off-policy learning (described briefly earlier in \ref{s:landscape}),
and model-based learning.
Learning off-policy allows for substantial sample efficiency gains because it is compatible
with experience replay. This mechanism consists in keeping a FIFO-queue buffer containing the
$N$ most recent transitions, and training the agent's policy (and value) on its contents.
It is \textit{off}-policy because the sampling distribution corresponding to getting transitions
uniformly from the replay buffer is a mixture of past policy updates, which is (in non-degenerate
cases) different from the policy being learned. Those methods are usually less friendly, but the
sample-efficiency gains are arguably well worth it in practical scenarios.

Model-based learning consists in learning a model of the environment in addition to learning
a policy (and value). It can be abbreviated MBRL for ``model-based RL''. What is modeled has
traditionally been the elements of the MDP, \textit{i.e.}
the forward dynamics $f: (s_t, a_t) \mapsto s_{t+1}$,
the reward process $r: (s_t, a_t) \mapsto r_t$,
and the termination flag (whether the episode is over or not) $d: (s_t, a_t) \mapsto d_t$.
This is because such a model can then act as a substitute for the real environment.
In that case, the interaction needs are lowered, and the method is more sample efficient
(technically). This category of MBRL approaches (replacing the environment with a learned one)
was introduced by Sutton with Dyna \cite{Sutton1991-cp}.

In addition to ``Dyna-style'' methods, there are ``SVG-style'' methods, where SVG refers to the
``Stochastic Value Gradients'' work \cite{Fairbank2012-rp, Heess2015-va, Amos2021-wd}.
The origin of this approach could be traced back to the appearance of
back-propagation-through-time (BPTT)
\cite{Rumelhart1986-ls, Robinson1987-px, Nguyen1990-zx, Werbos1990-qa,
Williams1990-xw, Jordan1992-wn, Grzeszczuk1998-ij}
(other possible name: ``pathwise derivatives''.)
It has then been continued in more modern setting by \cite{Schmidhuber2011-mt, Deisenroth2011-ya}, 
and very recently in the very successful DreamerV\{1-3\} methods
\cite{Hafner2019-oa, Hafner2021-td, Hafner2023-wk, Lin2023-ql}.
These methods learn a model of the world with neural networks so that the learned approximation of
the world can be part of the episode-spanning computational graph created by the RL objective.
Effectively, the gradient propagate backward through the trajectories, from the termination to
the initialization.

The third approach to MBRL consists in learning a model of the world and deriving a
policy by \emph{planning} on the model with a method like MPC (model-predictive control).
MPC is an online optimal control method
\cite{Bryson1969-mg, Bertsekas2000-yi, Kirk2004-dq}.
By virtue of being online, it is adaptable by design.
It might look closer to optimal control than RL but more modern
MPC-style methods use a learned \textit{termination value or critic}
for the learned controller to always be able to gauge the long-term of its plans, as in
TD-MPC2 \cite{Hansen2024-ld}.

For the three ways of conducting MBRL (Dyna-style, SVG-style, MPC-style),
it is common to learn models that do not follow the strict signatures of the MDP elements.
Instead, a more complex model is often learned, the forward model is derived from it if needed.
More complex models allow for more salient and holistic neural representations to be learned,
leading to better values and controller down the line. They are called \emph{world models},
which is an appropriately generic name considering how one might differ from one work to the next.
Some references to world models include
\cite{Ha2018-sa, Hafner2018-zm, Hessel2021-ah, Micheli2023-mr, Ding2024-oq, Alonso2024-jo}.
So:
\begin{description}
    \item[Dyna-style]
        The Dyna-style methods learn a model of the world to use this as an approximation
        instead of interacting with the simulator, hence \emph{technically} reducing how many times
        the agent must interact with the simulation to learn.
    \item[SVG-style]
        The SVG-style methods learn a \emph{neural} model of the world to use in the objective's
        computational graph and be part of the optimization problem directly,
        in the hope that leveraging world gradients will make the controller learn faster.
    \item[MPC-style]
        The MPC-style methods learn a model of the world to plan on, and planning is generally
        less costly in data points than model-free RL methods to derive a policy.
\end{description}

All three methods have their respective merits.
The most \textit{neural land} of the three is the SVG-style family.
It has had successes (see above) but mostly disappoints for the expected successes
it should have based on how paradigm-shifting end-to-end neural pipelines have been in essentially
all other sub-domains of machine learning.
The culprit is the behavior of the gradients of the world models in control scenarios
(let us call those \textit{world gradients}).
DreamerV\{1-3\} (SVG-style) has been extremely successful in \emph{discrete} control,
but TD-MPC2 (MPC-style) still yields better results in \emph{continuous} control.
This is likely due to the world gradients being tedious to deal with in continuous control tasks.
Backpropagation of annoying gradients (too big or too small) has been a hot topic for as long
as neural networks have existed ---see previous references cited above on BPTT.

A way to potentially get better world gradient is to use a \emph{differentiable simulator}.
We will abbreviate those as \textit{diffsims}.
Note that in such a case, there is no world model \textit{a priori}.
Instead of designing a learning task for a world model to extract useful and salient information
from the world by itself ---learning just what it \emph{needs} to solve the task,
the world gradients the agent gets are derived from hard-coded operations encoded via
deep learning frameworks.
As such, diffsims are a way to benefit from the sample efficiency of SVG-style model-based methods
without the need to learn anything else than the agent's policy (and value).
They are not \textit{learned from data} (note, \textit{where} this data comes from is a hurdle
of its own in world modeling) but are \textit{crafted} by the seasoned practitioner
who injects domain knowledge and well-vetted structural biases in it.
The diffsim designer has knowledge about the domain and knows how the design choices impact
the interaction time (direct consequence of the integration from dynamics to kinematics).
The designer can strike the desired trade-offs between fidelity with the real physics and speed.
More sophisticated models can lead to higher fidelity, but are more costly to integrate over.

Plus, diffsims are hard-coded and therefore less prone to \textit{model hacking} (also refered
to as \textit{model exploitation} or \textit{model bias}).
This phenomenon is observed when the policy uses the model of the world in ways that were
unintended by the practitioner. It \textit{hacks} or \text{exploits} the model to solve the task
it was asked to solve, but did it in unforeseen ways at test time
\cite{Ross2012-yj}.
This is typical when dealing with black boxes.
The phenomenon is close in spirit to reward hacking.
It is easier to correct in diffsims simply because, since the simulator is hard-coded and readable,
we can \textit{know} what was exploited. This part of the pipeline is a white box.
Model exploitation loopholes are especially critical to avoid in SVG-style methods since
feedback propagate along trajectories and pollutes everytime upstream if erroneous.
Also, locomotion simulators are tedious to turn into diffsims notably because of the
\emph{contacts}, which introduce discontinuities in the simulator's response, and therefore
Dirac deltas in the diffsim gradients.
Design choices with respect to contact design are therefore particularly critical.
Harder contacts are better for fidelity and therefore closing the sim-to-real gap,
but softer contacts (dampened, surfaces like ground being permeable) remove the risk of introducing
spikes in world gradients.
Another point of design: in order to back-propagate from the RL objective through the diffsim
backward through long trajectories, the reward itself need be a differentiable signal
with respect to the quantities of interest, which is not necessarily obvious.
In the case of IL however, the displacement error need only the dynamics to be differentiable.
Lastly, since diffsims are tedious to engineer, using them is often an exercise in patience.
Here are relevant references on diffsims:
\cite{Degrave2016-vr, Hu2019-ax, Falisse2019-ge, Hu2020-eb, Qiao2020-um, Gradu2020-kq,
Geilinger2020-ar, Mora2021-pp, Heiden2021-fb, Clarke2021-rr, Werling2021-da, Qiao2021-zn,
Huang2021-le, Heiden2021-ey, Grinsztajn2021-mr, Daniel_Freeman2021-az, Lin2022-jr, Xu2022-bz,
Suh2022-mn, Allen2022-vc, Chen2022-zd, Ren2023-yc, Georgiev2024-rs}.

\subsection{Execution Speed}
\label{ss:speed}

TODO

\begin{comment}
domain randomization very useful to be able to communicate to the agent what is relevant and what
    is not; by seeing a variety of textures and colors that do not change the task in nature, we
    want the model to understand what it should be invariant to (cf. literature on self-supervised
    learning)
    SO, if one want to close the gap, showing lots of things to the agent is crucial, and for that
    to be feasible, the sim need be fast

problem of character animation in rendering engine is almost identical
    an advantage they have is that game engines do not car as much about contacts being sharp
    usage of dampeners that make contact smooth --making them technically infringe physics
    those are usually very fast and done by game studios (so run on GPU), but clear trade-off

loco sims that are high fidelity and well-vetted are all CPU-based
    GPU-based sims are considerably faster but some operations in locomotion model primitives
    can not be done via big matrix multiplications and are therefore not able to leverage the
    capabilities of GPU, which is why those sims have not been ported to GPU as of yet
    BRAX/MLX might be getting there very soon, so it is very important to keep an eye on those
    dlex from Nvidia, warp
    again: the more we go high-fidelity with real world, the better we can close the sim2real gap

muscles
    OpenSim: CPU
    dflex: GPU
\end{comment}

\section{Best Candidate}
\label{s:best}

TODO

\begin{comment}
====extra things====
S4 etc.

TD-MPC2 >> DreamerV3 for continuous control (DreamerV3 better for discrete control,
    although allegedly worse now than IRIS, Diamond, TWM, etc.)
this requires to learn a world model, to then plan on

MoCap
Temporal Cycle-Consistency Learning (TCC)
in-context learning (in-context RL --ICRL)
\end{comment}

% References
\bibliographystyle{plain}
\bibliography{my_bib} % (without the .bib extension)

\end{document}
